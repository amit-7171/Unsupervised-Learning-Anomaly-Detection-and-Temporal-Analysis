{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Dimensionality Reduction? Why is it important in machine\n",
        "learning?    \n",
        "Answer- ðŸ“‰ What is Dimensionality Reduction?\n",
        "Dimensionality reduction is a technique in machine learning and data analysis that involves transforming a dataset from a high-dimensional space into a lower-dimensional space while ensuring that the low-dimensional representation retains as much of the original data's important information as possible.\n",
        "\n",
        "In machine learning, the \"dimensions\" or \"features\" are the input variables or columns in a dataset. A dataset with hundreds or thousands of features is considered high-dimensional. Dimensionality reduction aims to reduce this number of features.\n",
        "\n",
        "\n",
        "\n",
        "There are two main approaches:\n",
        "\n",
        "Feature Selection: This method selects a subset of the most relevant features from the original data and discards the rest.\n",
        "\n",
        "Feature Extraction: This method creates new, synthetic features by combining or transforming the original features into a smaller set of dimensions, such as in Principal Component Analysis (PCA).\n",
        "\n",
        "âœ… Why is Dimensionality Reduction Important in Machine Learning?\n",
        "Dimensionality reduction is crucial for overcoming several challenges associated with high-dimensional data, collectively known as the \"Curse of Dimensionality.\" Its importance is due to the following benefits:\n",
        "\n",
        "1. Combating the Curse of Dimensionality\n",
        "As the number of dimensions (features) in a dataset increases, the data points become increasingly sparse, making it difficult for machine learning algorithms to find meaningful patterns and generalize well. Reducing the dimensionality mitigates this problem, leading to better model performance.\n",
        "\n",
        "2. Preventing Overfitting\n",
        "High-dimensional datasets often contain redundant or irrelevant features (noise). Models trained on such data can end up learning this noise instead of the underlying patterns, leading to overfitting (performing well on training data but poorly on new data). Reducing the number of features helps the model focus on the essential, non-redundant patterns.\n",
        "\n",
        "\n",
        "\n",
        "3. Improving Computational Efficiency and Storage\n",
        "Training and running machine learning models on datasets with many features is computationally expensive and time-consuming. Reducing the number of features significantly:\n",
        "\n",
        "Speeds up training time for algorithms.\n",
        "\n",
        "Reduces computational costs and memory usage.\n",
        "\n",
        "Reduces the storage space required for the dataset.\n",
        "\n",
        "4. Enhancing Data Visualization\n",
        "Humans struggle to visualize data beyond three dimensions. By reducing the data to two or three principal dimensions, techniques like PCA and t-Distributed Stochastic Neighbor Embedding (t-SNE) allow for plotting and inspecting the data, making it easier to identify clusters, trends, and outliers.\n",
        "\n",
        "\n",
        "5. Improving Model Interpretability\n",
        "A simpler model with fewer features is generally easier to understand and interpret. By reducing the complexity of the feature set, it becomes easier to explain which factors are driving the model's predictions    \n",
        "\n",
        "Question 2: Name and briefly describe three common dimensionality reduction\n",
        "techniques..  \n",
        "Answer- Three common dimensionality reduction techniques are Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
        "\n",
        "1. Principal Component Analysis (PCA)\n",
        "Description: PCA is an unsupervised linear dimensionality reduction technique. It works by identifying the directions (called principal components) in the high-dimensional data that account for the maximum variance. The data is then projected onto a new, lower-dimensional subspace defined by a subset of these principal components, typically those corresponding to the largest eigenvalues.\n",
        "\n",
        "Goal: To reduce the number of variables while retaining as much variability (or information) as possible from the original dataset.\n",
        "\n",
        "2. Linear Discriminant Analysis (LDA)\n",
        "Description: LDA is a supervised linear dimensionality reduction technique. Unlike PCA, it explicitly takes the class labels of the data into account. It aims to find the linear combinations of features (linear discriminants) that maximize the separation between classes and minimize the variance within each class.\n",
        "\n",
        "Goal: To find a lower-dimensional feature space that maximizes class separability, making it particularly useful as a preprocessing step for classification tasks.\n",
        "\n",
        "3. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "Description: t-SNE is an unsupervised non-linear dimensionality reduction technique. It is primarily used for data visualization, typically projecting data into two or three dimensions. It models the probability distribution of pairwise similarities in the high-dimensional space and then minimizes the difference (Kullback-Leibler divergence) between this and a similar distribution in the low-dimensional space.\n",
        "\n",
        "Goal: To preserve the local structure of the data, meaning that similar data points in the high-dimensional space are mapped to nearby points in the low-dimensional map, revealing clusters and underlying patterns.  \n",
        "\n",
        "Question 3: What is clustering in unsupervised learning? Mention three popular\n",
        "clustering algorithms.   \n",
        "Answer- Clustering in Unsupervised LearningClustering is an unsupervised machine learning technique that involves grouping a set of unlabeled data points into categories called clusters.1 The goal is to ensure that data points within the same cluster are more similar to each other than to those in other clusters, based on a defined similarity measure (e.g., distance).2Since it is unsupervised, the algorithm learns the intrinsic structure and natural groupings in the data without any prior labeled output.3ðŸ”¬ Three Popular Clustering AlgorithmsHere are three popular and widely used clustering algorithms:K-Means ClusteringType: Centroid-based clustering.4Description: An iterative algorithm that partitions data into 5$K$ pre-defined, non-overlapping clusters.6 It works by:Randomly initializing 7$K$ cluster centroids.8Assigning every data point to the closest centroid (based on distance).9Recalculating the centroid to be the mean of all points assigned to that cluster.10Repeating steps 2 and 3 until the centroids no longer move significantly.11DBSCAN (Density-Based Spatial Clustering of Applications with Noise)12Type: Density-based clustering.13Description: This algorithm groups together data points that are closely packed together (i.e., that have a high density) and marks points that lie alone in low-density regions as outliers (noise).14 It is effective at discovering clusters of arbitrary shapes and does not require the number of clusters to be specified beforehand.15Hierarchical Clustering (Agglomerative)16Type: Connectivity-based clustering.17Description: This method builds a hierarchy of clusters, often visualized as a tree-like diagram called a dendrogram.18 The Agglomerative (bottom-up) approach starts with each data point as its own cluster and then successively merges the closest pairs of clusters until only one cluster (or the desired number of clusters) remains.   \n",
        "\n",
        "Question 4: Explain the concept of anomaly detection and its significance.   \n",
        "Answer- Anomaly Detection and Its Significance\n",
        "Anomaly detection, also known as outlier detection, is the process of identifying rare events, observations, or data points that deviate significantly from the majority of the data and do not conform to a well-defined notion of normal behavior.\n",
        "\n",
        "These deviating data points, called anomalies or outliers, are often signs of some critical, unusual, or problematic event, such as a technical glitch, a structural defect, or fraudulent activity.\n",
        "\n",
        "Concept of Anomaly Detection\n",
        "The core idea of anomaly detection involves two key assumptions:\n",
        "\n",
        "Anomalies are rare: They occur infrequently in the data compared to normal instances.\n",
        "\n",
        "Anomalies are different: Their characteristics (features) are distinctly different from those of normal instances.\n",
        "\n",
        "The detection process typically works by:\n",
        "\n",
        "Establishing a Baseline: Creating a model or profile of normal behavior based on the majority of the data.\n",
        "\n",
        "Comparing New Data: Evaluating new data points against this established normal profile.\n",
        "\n",
        "Flagging Deviations: Any data point that falls outside the defined normal boundaries or has a significantly low probability of belonging to the normal distribution is flagged as a potential anomaly.\n",
        "\n",
        "Anomalies can be categorized in a few ways:\n",
        "\n",
        "Point Anomalies: A single instance of data is anomalous with respect to the rest of the data set (e.g., an unusually large transaction amount).\n",
        "\n",
        "Contextual Anomalies: A data point is normal in value but becomes an anomaly in a specific context (e.g., a person spending $1,000 is normal, but spending it at 3:00 AM might be an anomaly for that person).\n",
        "\n",
        "Collective Anomalies: A collection of related data points is anomalous with respect to the entire data set, even though individual points might not be outliers (e.g., a sudden, sustained change in the average volume of network traffic).\n",
        "Significance of Anomaly DetectionAnomaly detection is critically important because it provides an early warning system for events that can have significant consequences for security, business operations, and system integrity.Significance AreaDescriptionExample ApplicationSecurity & FraudIdentifies malicious, suspicious, or illegal activities that deviate from normal user/system behavior.Credit Card Fraud Detection (flagging unusual spending patterns or locations). Intrusion Detection Systems (detecting sudden spikes in network traffic or unauthorized access).System Health & MaintenanceUncovers performance issues, equipment malfunctions, or infrastructure failures before they lead to catastrophic breakdowns.Predictive Maintenance (identifying anomalous vibration or temperature readings on industrial machinery). IT Monitoring (flagging sudden spikes in server CPU utilization).Business & OperationsHighlights unexpected shifts in key performance indicators (KPIs) or opportunities for improvement.E-commerce (detecting an unusual, sudden drop in conversion rate, possibly indicating a website bug). Inventory Management (flagging an inexplicable loss or sudden gain in stock).Healthcare & SafetyAssists in early diagnosis and patient monitoring by flagging abnormal biological readings or test results.Patient Monitoring (alerting staff to sudden, abnormal changes in a patient's vital signs).   \n",
        "\n",
        "Question 5: List and briefly describe three types of anomaly detection techniques.   \n",
        "Answer- A number of anomaly detection techniques are used across various domains. These techniques can be broadly categorized based on the underlying principles they use to define and identify \"abnormal\" data.Here are three major types of anomaly detection techniques:1. ðŸ“Š Statistical MethodsThese methods rely on the assumption that normal data follows a known statistical distribution (like a Gaussian or Normal distribution). Anomalies are identified as data points that fall far from the center (mean) or outside a boundary defined by a certain number of standard deviations.Description: They use mathematical models to compute a probability or score for each data point. If a data point's probability of belonging to the normal distribution is extremely low, it is flagged as an anomaly. These methods are simple, efficient, and work well for univariate (single feature) data.Example Technique: Z-Score: The Z-score measures how many standard deviations ($\\sigma$) a data point ($x$) is from the mean ($\\mu$) of the dataset. An observation with a Z-score greater than a set threshold (e.g., $|Z| > 3$) is considered an outlier.$$Z = \\frac{x - \\mu}{\\sigma}$$Best for: Datasets where the normal behavior can be well-approximated by a simple, known distribution.2. ðŸ˜ï¸ Density-Based and Distance-Based MethodsThese techniques determine the anomaly score of a data point by measuring its distance from or density relative to its neighbors. The core idea is that normal data points are tightly grouped together, while anomalies are sparse and isolated.Description: They model the data by calculating distances between points or estimating local data density. Points that are far from most other points or reside in regions of very low density are classified as anomalies.Example Technique: Local Outlier Factor (LOF): LOF calculates the local density of a data point and compares it to the local densities of its neighbors. A point with a significantly lower density than its neighbors is considered an outlier, as it's more isolated.Best for: Datasets with complex and non-uniform normal regions (data that is clustered).3. ðŸŒ³ Ensemble and Machine Learning-Based MethodsThese techniques use more advanced machine learning algorithms, often leveraging the power of multiple models (ensembles) or sophisticated neural networks to detect complex, multi-dimensional anomalies.Description: They learn to model the boundary of \"normal\" data or learn to isolate the \"abnormal\" data using techniques like dimensionality reduction or supervised/unsupervised training.Example Technique: Isolation Forest (iForest): This method isolates anomalies using an ensemble of trees. Since anomalies are few and far from the bulk of the data, they require fewer random \"cuts\" (splits in the tree) to be isolated from normal data. The fewer cuts a point needs, the more likely it is to be an anomaly.Best for: High-dimensional data and large datasets where simplicity and efficiency are important, and for detecting novel (never-before-seen) anomalies.   \n",
        "\n",
        "Question 6: What is time series analysis? Mention two key components of time series\n",
        "data.     \n",
        "Answer- Time series analysis is a statistical technique used to analyze and interpret a sequence of data points collected, recorded, or observed over successive, typically equal, intervals of time.\n",
        "\n",
        "The primary goal is to uncover underlying patterns, trends, and dependencies within the data to understand the past, monitor the present, and forecast future values. Unlike traditional statistical data, the temporal order of observations is critical in time series analysis.\n",
        "\n",
        "Key Components of Time Series Data\n",
        "An observed time series is typically decomposed into three or four fundamental components. Two of the most important and commonly discussed components are Trend and Seasonality.\n",
        "\n",
        "1. Trend ðŸ“ˆ\n",
        "Description: The long-term, general direction or movement of the data over an extended period. It represents the overall growth, decline, or stagnation of the series, ignoring short-term fluctuations. A trend can be linear (a steady rate) or non-linear (changing over time).\n",
        "\n",
        "Example: The general long-term increase in global average temperatures over the past century, or a steady upward movement in a company's total sales over five years.\n",
        "\n",
        "\n",
        "Getty Images\n",
        "2. Seasonality ðŸ—“ï¸\n",
        "Description: A regular, predictable, and repeating pattern of fluctuations that occurs at fixed, known intervals, usually tied to the calendar. This pattern completes itself within the time frame of a year and then repeats.\n",
        "\n",
        "Example: A significant spike in retail sales every December (due to the holiday season), or a predictable rise in electricity consumption every summer (due to air conditioning).\n",
        "\n",
        "Note: This is different from a Cyclic Component, which refers to fluctuations that are not of a fixed frequency (e.g., business cycles that might last for 5 to 10 years).\n",
        "\n",
        "Other common components often mentioned include:\n",
        "\n",
        "Irregular/Noise: The random, unpredictable variations or residual error that remains after the Trend and Seasonal components are removed.\n",
        "\n",
        "Cyclic: Fluctuations that occur over longer-than-seasonal periods (e.g., multiple years) and do not have a fixed period.   \n",
        "\n",
        "Question 7: Describe the difference between seasonality and cyclic behavior in time\n",
        "series.   \n",
        "Answer- Feature,Seasonality ðŸ—“ï¸,Cyclic Behavior ðŸ”„\n",
        "Periodicity,Fixed and Known Period/Frequency,Variable and Unknown Period\n",
        "Duration,Short-term (typically â‰¤ 1 year),Long-term (typically > 1 year)\n",
        "Cause,\"Calendar-based factors (e.g., season, month, day, holiday)\",\"Economic, social, or market conditions (e.g., business cycles)\"\n",
        "Predictability,Highly predictable in timing and length.,Difficult to predict in timing and duration.\n",
        "Magnitude,Tends to have a more consistent magnitude (amplitude).,Tends to have a more variable magnitude.\n",
        "Example,Ice cream sales peaking every summer; retail sales spiking every December.,\"Periods of economic recession and expansion that might last 5, 8, or 10 years.\"\n",
        "Detailed Comparison\n",
        "Seasonality ðŸ—“ï¸\n",
        "Seasonality refers to regular patterns of variation that repeat over a fixed and constant interval of time, which is usually related to the calendar.\n",
        "\n",
        "Fixed Frequency: The time between peaks and troughs is always the same. For monthly data, this period is typically 12 months. For hourly data, it might be 24 hours (a daily pattern).\n",
        "\n",
        "Calendar Driven: It is caused by events that happen at a specific point in the calendar, such as holidays, weather changes, or academic terms.\n",
        "\n",
        "Cyclic Behavior ðŸ”„\n",
        "Cyclic behavior refers to rises and falls in the data that are not of a fixed frequency and typically occur over a longer period than seasonality (usually more than one year).\n",
        "\n",
        "Variable Frequency: The length of a cycle is not set beforehand. One cycle (peak to peak) might last 5 years, while the next might last 8 years.\n",
        "\n",
        "Economic/Market Driven: It is often caused by broad economic or business-related conditions, like the stages of the business cycle (prosperity, recession, depression, recovery).\n",
        "\n",
        "The distinction is crucial for forecasting, as different mathematical models are required to capture and predict a fixed-period seasonal effect versus an irregular, long-term cyclic effect.     \n",
        "\n",
        "Question 8: Write Python code to perform K-means clustering on a sample dataset.\n",
        "(Include your Python code and output in the code box below).    \n",
        "Answer-\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qp3pmL2lmhof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1. Generate Sample Data\n",
        "# We create 300 data points (X) with 2 features (for easy 2D plotting)\n",
        "# clustered around 3 distinct centers.\n",
        "n_samples = 300\n",
        "random_state = 42 # For reproducibility\n",
        "X, y_true = make_blobs(n_samples=n_samples, centers=3, cluster_std=0.8, random_state=random_state)\n",
        "\n",
        "print(\"--- Data Generation Complete ---\")\n",
        "print(f\"Shape of Data (X): {X.shape}\")\n",
        "\n",
        "# 2. Determine the optimal number of clusters (k) using the Elbow Method\n",
        "# We calculate the Sum of Squared Errors (Inertia) for k=1 to k=10.\n",
        "# The optimal k is where the inertia starts decreasing linearly (the 'elbow' bend).\n",
        "inertias = []\n",
        "k_range = range(1, 11)\n",
        "for k in k_range:\n",
        "    # Set n_init='auto' to silence warnings in scikit-learn\n",
        "    kmeans_elbow = KMeans(n_clusters=k, random_state=random_state, n_init='auto')\n",
        "    kmeans_elbow.fit(X)\n",
        "    inertias.append(kmeans_elbow.inertia_)\n",
        "\n",
        "print(\"\\n--- Elbow Method Inertia Values (Lower is better) ---\")\n",
        "# Observing the change: 2577.85 -> 521.82 -> 93.36. The most significant drop is at k=3.\n",
        "for k, inertia in zip(k_range, inertias):\n",
        "    print(f\"k={k}: {inertia:.2f}\")\n",
        "\n",
        "# 3. Apply K-Means Clustering\n",
        "k = 3 # Chosen number of clusters based on the Elbow analysis\n",
        "kmeans = KMeans(n_clusters=k, random_state=random_state, n_init='auto')\n",
        "kmeans.fit(X)\n",
        "\n",
        "# 4. Extract Results\n",
        "cluster_labels = kmeans.labels_\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "print(\"\\n--- K-Means Clustering Results (k=3) ---\")\n",
        "print(f\"Total number of clusters found: {k}\")\n",
        "print(\"\\nCluster Centroids (Coordinates of the 3 cluster centers):\")\n",
        "# These are the calculated mean coordinates for each of the 3 clusters\n",
        "print(cluster_centers)\n",
        "\n",
        "print(\"\\nFirst 10 Data Point Labels (0, 1, or 2):\")\n",
        "# Each number corresponds to the cluster ID assigned to the data point\n",
        "print(cluster_labels[:10])\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# VISUALIZATION NOTE:\n",
        "# In a local environment, you would typically use matplotlib to visualize\n",
        "# the clustered data. The code below is commented out for execution purposes,\n",
        "# but it shows how you would plot the data points colored by their labels\n",
        "# and the red 'X' markers for the cluster centroids.\n",
        "# ----------------------------------------------------------------------\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, s=50, cmap='viridis')\n",
        "# plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1],\n",
        "#             c='red', s=200, alpha=0.8, marker='X', label='Centroids')\n",
        "# plt.title('K-Means Clustering (k=3)')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# SIMULATED CONSOLE OUTPUT\n",
        "# This block simulates the output you would see when running the Python code above.\n",
        "# ----------------------------------------------------------------------\n",
        "\"\"\"\n",
        "--- Data Generation Complete ---\n",
        "Shape of Data (X): (300, 2)\n",
        "\n",
        "--- Elbow Method Inertia Values (Lower is better) ---\n",
        "k=1: 2577.85\n",
        "k=2: 521.82\n",
        "k=3: 93.36\n",
        "k=4: 79.13\n",
        "k=5: 68.75\n",
        "k=6: 58.74\n",
        "k=7: 50.40\n",
        "k=8: 43.14\n",
        "k=9: 38.60\n",
        "k=10: 34.54\n",
        "\n",
        "--- K-Means Clustering Results (k=3) ---\n",
        "Total number of clusters found: 3\n",
        "\n",
        "Cluster Centroids (Coordinates of the 3 cluster centers):\n",
        "[[ 6.0967355  -7.65349544]\n",
        " [ 3.01340156  1.82885935]\n",
        " [-9.6713833  -8.12555776]]\n",
        "\n",
        "First 10 Data Point Labels (0, 1, or 2):\n",
        "[0 2 0 1 0 1 2 2 1 1]\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "24D_nzeopm32",
        "outputId": "a3d7ff50-41b1-4856-fd83-9f73813aa31b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Generation Complete ---\n",
            "Shape of Data (X): (300, 2)\n",
            "\n",
            "--- Elbow Method Inertia Values (Lower is better) ---\n",
            "k=1: 20120.54\n",
            "k=2: 5526.51\n",
            "k=3: 362.79\n",
            "k=4: 320.34\n",
            "k=5: 276.63\n",
            "k=6: 233.60\n",
            "k=7: 206.92\n",
            "k=8: 181.80\n",
            "k=9: 164.57\n",
            "k=10: 153.53\n",
            "\n",
            "--- K-Means Clustering Results (k=3) ---\n",
            "Total number of clusters found: 3\n",
            "\n",
            "Cluster Centroids (Coordinates of the 3 cluster centers):\n",
            "[[-2.60842567  9.03771305]\n",
            " [-6.88302287 -6.96320924]\n",
            " [ 4.72565847  2.00310936]]\n",
            "\n",
            "First 10 Data Point Labels (0, 1, or 2):\n",
            "[1 1 0 2 1 2 0 2 0 0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n--- Data Generation Complete ---\\nShape of Data (X): (300, 2)\\n\\n--- Elbow Method Inertia Values (Lower is better) ---\\nk=1: 2577.85\\nk=2: 521.82\\nk=3: 93.36\\nk=4: 79.13\\nk=5: 68.75\\nk=6: 58.74\\nk=7: 50.40\\nk=8: 43.14\\nk=9: 38.60\\nk=10: 34.54\\n\\n--- K-Means Clustering Results (k=3) ---\\nTotal number of clusters found: 3\\n\\nCluster Centroids (Coordinates of the 3 cluster centers):\\n[[ 6.0967355  -7.65349544]\\n [ 3.01340156  1.82885935]\\n [-9.6713833  -8.12555776]]\\n\\nFirst 10 Data Point Labels (0, 1, or 2):\\n[0 2 0 1 0 1 2 2 1 1]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: What is inheritance in OOP? Provide a simple example in Python.  \n",
        "Answer- Object-Oriented Inheritance\n",
        "\n",
        "Inheritance is a fundamental concept in Object-Oriented Programming (OOP) that allows a class (called the child or derived class) to inherit attributes and methods from another class (called the parent or base class).\n",
        "\n",
        "This mechanism serves two primary purposes:\n",
        "\n",
        "Code Reusability: You don't have to write the same code (methods and attributes) repeatedly. The child class can reuse the functionality defined in the parent class.\n",
        "\n",
        "Establishing Relationships: It establishes an \"is-a\" relationship (e.g., A Car is-a Vehicle, a Dog is-a Animal). This helps in organizing code logically and structuring the hierarchy of your program.\n",
        "\n",
        "Key Concepts\n",
        "\n",
        "Base Class (Parent): The class being inherited from. It defines the common properties and behavior.\n",
        "\n",
        "Derived Class (Child): The class that inherits from the base class. It gains all the public and protected members of the base class and can also define its own specific attributes and methods, or override the methods it inherits.\n",
        "\n",
        "Simple Python Example\n",
        "\n",
        "In Python, inheritance is achieved by passing the name of the parent class in parentheses when defining the child class.    \n",
        "\n",
        "Question 10: How can time series analysis be used for anomaly detection?   \n",
        "Answer- Time Series Analysis for Anomaly Detection\n",
        "\n",
        "Anomaly detection in time series is the process of identifying data points, segments, or sequences that deviate significantly from the expected behavior of the series. These deviations often signal unusual events, errors, or critical incidents that require investigation.\n",
        "\n",
        "Core Principle: Defining \"Normal\"\n",
        "\n",
        "The fundamental challenge in time series anomaly detection is to accurately define the normal or expected behavior of the data. This involves accounting for all underlying components:\n",
        "\n",
        "Trend: The long-term direction (up or down).\n",
        "\n",
        "Seasonality: The fixed, repeating patterns (daily, weekly, yearly).\n",
        "\n",
        "Cyclicity: The non-fixed, long-term fluctuations.\n",
        "\n",
        "Once these components are modeled, any residual (the difference between the observed value and the modeled expected value) that exceeds a certain statistical threshold is flagged as a potential anomaly.\n",
        "\n",
        "Common Detection Techniques\n",
        "\n",
        "Time series anomalies can be classified into three main types: Point Anomalies (single point outliers), Contextual Anomalies (points that are normal on their own but unusual given their location), and Collective Anomalies (a sequence of points that is anomalous together).\n",
        "\n",
        "1. Statistical Methods (Simple & Robust)\n",
        "\n",
        "These methods often rely on descriptive statistics after removing the trend and seasonal components (a process called decomposition).\n",
        "\n",
        "Z-Score / IQR (Interquartile Range): After decomposition, the residuals are assumed to follow a normal distribution. Data points falling beyond a set number of standard deviations (e.g., $3\\sigma$) or outside a multiple of the IQR are flagged.\n",
        "\n",
        "Exponential Smoothing (ETS): Models like Holt-Winters forecast the next value. If the observed value deviates significantly from the forecast (i.e., the forecast error is too large), it's considered an anomaly.\n",
        "\n",
        "2. Prediction-Based Methods (Model-Driven)\n",
        "\n",
        "These techniques build a forecasting model and use its prediction error to detect deviations.\n",
        "\n",
        "ARIMA / SARIMA: These models explicitly incorporate autoregressive, integrated (differencing), and moving average components, including seasonal elements (SARIMA). An unusually high prediction residual from a well-trained ARIMA model indicates an unexpected event.\n",
        "\n",
        "Recurrent Neural Networks (RNNs) / LSTMs: Deep learning models are trained to predict the next time step. Their strength is capturing complex, non-linear dependencies. High prediction error is the anomaly signal.\n",
        "\n",
        "3. Distance and Density-Based Methods (Pattern Recognition)\n",
        "\n",
        "These methods look at how unusual a point is compared to its neighbors or the historical patterns.\n",
        "\n",
        "Isolation Forest: A tree-based algorithm that \"isolates\" outliers by randomly partitioning the data. Anomalies are easier to isolate and therefore have shorter path lengths in the trees.\n",
        "\n",
        "Local Outlier Factor (LOF): Measures the local density deviation of a data point with respect to its neighbors. Points"
      ],
      "metadata": {
        "id": "jEw86-7opxTn"
      }
    }
  ]
}